<html>
<head>
<link rel="stylesheet"  type="text/css" href="help.css">
</head>
<body>

<!-- HEADER -->

<h1>Object Detection</h1>

One of the first steps in image processing is <i>object detection</i>, which can be described as a partitioning of an image into regions of interest (ROI) and the background. Every ROI is referred here as <i>object</i> and has a unique number within one image and for one timepoint.<br>
Parameters for object detection are divided into <i>primary</i> and <i>secondary</i> channel. For the primary channel conversion from 16 to 8 bit, local adaptive thresholding, watershed and object filtering can be specified.
Register shift and regions derived from the primary segmentation can be defined for the secondary channel (see <img class="extlink" src=":extlink"/><a href="http://www.cellcognition.org/wiki/CecogAnalyzer">screenshots</a>). Contour images are shown in separate window during processing.

<hr/>
<h4>Workflow for classification in two channels</h4>
<br/>
<img class="figure" src=":/cecog_workflow"/>
<hr/>
<h4>Workflow for classification in the primary and feature readout in the secondary channel</h4>
<br/>
<img class="figure" src=":/cecog_workflow_kinetic"/>
<hr/>

<h2 class="param">Primary Channel</h2>

Objects are detected in the <i>primary channel</i>. The methods presented here have been developed primarily for images of fluorescently labeled cell nuclei (stained by DAPI or labeled with H2B).

<hr></hr>
<h3 class="param">Parameters</h3>

<a name="primary_channelid">
<h4>Primary channel ID</h4>
<p class="text">The identifier of the channel as specified in the meta-data. For now this information is extracted from the filename and depends on how the naming-schema interprets the filename, e.g. a filename with the token <tt>__Crfp__</tt> corresponds to the channel ID <tt>rfp</tt> in our demo data</p>
</a>

<a name="primary_channel_conversion">
<h4>16 to 8 bit conversion</h4>
<p class="text">Microscope CCD cameras acquire images in 12 bit (4096 gray values), which in fact are saved as 16 bit images. Our image processing requires 8 bit (256 gray values) input images, which was sufficient for the methods presented here. 16 bit images will be converted to 8 bit by <i>linear range mapping</i> with the values below (in that case values must be specified). 16 bit (values 0-65535) are mapped to 8 bit (values 0-255).<br>8 bit input images are not affected.</p>
</a>
<a name="primary_normalizemin">
<h4 class="sub">Min.</h4>
<p class="subtext">Lower range value. (The value in the 16 bit image which corresponds to 0 in the 8 bit image).</p>
</a>
<a name="primary_normalizemax">
<h4 class="sub">Max.</h4>
<p class="subtext">Higher range value (The value in the 16 bit image which corresponds to 255 in the 8 bit image).</p>
</a>

<hr></hr>

<h3 class="param">Z-stacks</h3>
<p class="mtext">In case z-stacks have been acquired the 3D data has to be converted to 2D before object detection. Either a z-slice can be selected or a projection is computed on the z-stack.</p>
<a name="primary_zslice_selection">
<h4>Z-slice selection</h4>
<p class="text">One slice of the z-stack will be selected.</p>
</a>
<a name="primary_zslice_selection_slice">
<h4 class="sub">Slice</h4>
<p class="subtext">The selected z-stack slice.</p>
</a>

<a name="primary_zslice_projection">
<h4>Z-slice projection</h4>
<p class="text">Compute a projection of the z-stack.</p>
</a>
<a name="primary_zslice_projection_method">
<h4 class="sub">Method</h4>
<p class="subtext">Computation method, either <i>average intensity</i> or <i>maximum intensity</i> projection. The projection can be computed from a subset of the z-stack, which is specified below: </p>
</a>
<a name="primary_zslice_projection_begin">
<h4 class="sub">Begin</h4>
<p class="subtext">Lowest slice to be included (smallest number).</p>
</a>
<a name="primary_zslice_projection_end">
<h4 class="sub">End</h4>
<p class="subtext">Highest slice to be included (highest number).</p>
</a>
<a name="primary_zslice_projection_step">
<h4 class="sub">Step</h4>
<p class="subtext">Step-size.
<br>Examples for z-stack with 10 slices:
<ul>
<li>Begin=3, End=10, Step=1 -> z-slices 3-10 are projected</li>
<li>Begin=1, End=10, Step=2 -> z-slices 1,3,5,7,9 are projected</li>
<li>Begin=2, End=8, Step=3 -> z-slices 2,5,8 are projected</li>
</ul>
</p>
</a>

<a name="primary_flat_field_correction">
<h4 class="text">Flat-field correction</h4>
<p class="text">Select a directory to search for the correction images(s). Convention is one image per plate. The full path of an image is <i>&lt;directory-path&gt;/&lt;plate-id&gt;.tif</i>. Note that only tif images are supported.</p>
</a>


<hr></hr>

<a name="primary_medianradius">
<h4>Median radius</h4>
<p class="text">Intensity of the image smoothing before object detection by a median filter (radius size in pixel). Note: Filtered image is used for object detection only (no influence on feature extraction, but shape features might be affected).
<br>(parameter scales directly with lens magnification)</p>
</a>

<a name="primary_lat">
<h4>Local adaptive threshold</h4>
<p class="text">Performs object detection based on local adaptive thresholding. A background map of the input image is computed based on the <i>Window size</i> (moving average window). Image pixels higher above the background map than <i>Min. contrast</i> are considered foreground, all other pixels background.</p>
</a>
<a name="primary_latwindowsize">
<h4 class="sub">Window size</h4>
<p class="subtext">Width of the moving average window in pixel (window is squared).
<br>(parameter scales directly with lens magnification)</p>
</a>
<a name="primary_latlimit">
<h4 class="sub">Min. contrast</h4>
<p class="subtext">Pixel intensity above the background. Note: Small values might increase artifacts (small objects from high noise levels) and slow down computation; high values might lead to incorrect contours.</p>
</a>

<a name="primary_lat2">
<h4>Local adaptive threshold 2</h4>
<p class="text">Optional second <a href="primary_lat">local adaptive threshold</a> to overcome the problem of incorrect object contours when very bright and very dark objects are in close spatial proximity.
See <img class="extlink" src=":extlink"/><a href="http://linkinghub.elsevier.com/retrieve/pii/S1047-8477(09)00273-1">Walter et al. 2010</a>.</p>
</a>
<a name="primary_latwindowsize2">
<h4 class="sub">Window size</h4>
<p class="subtext">Recommended are 4-11x higher values then above.</p>
</a>
<a name="primary_latlimit2">
<h4 class="sub">Min. contrast</h4>
<p class="subtext">Recommended are 3-4x higher values than above.</p>
</a>

<a name="primary_shapewatershed">
<h4>Split & merge by shape</h4>
<p class="text">Optional correction of under-segmentation by a split and merge approach. Objects in close spatial proximity might not be separated (under-segmentation). Based on the shape information (distance transform) objects are split by the <i>watershed</i> algorithm, which often yields over-segmentation. Object candidates are probed for merging by their size and circularity (rounder objects above a size threshold are preferred).
See <img class="extlink" src=":extlink"/><a href="http://www3.interscience.wiley.com/journal/118757015/abstract">W&auml;hlby et al. 2004</a>.</p>
</a>
<a name="primary_shapewatershed_gausssize">
<h4 class="sub">Gauss radius</h4>
<p class="subtext">Size of the Gauss filter for smoothing the distance transformed image.
<br>(parameter scales directly with lens magnification)</p>
</a>
<a name="primary_shapewatershed_maximasize">
<h4 class="sub">Min. seed distance</h4>
<p class="subtext">Minimal allowed distance in pixel between objects (seed distance for watershed).
<br>(parameter scales directly with lens magnification)</p>
</a>
<a name="primary_shapewatershed_minmergesize">
<h4 class="sub">Object size threshold</h4>
<p class="subtext">Minimal size (pixel) of a split object.
<br>(parameter scales square with lens magnification)</p>
</a>

<a name="primary_postprocessing">
<h4>Object filter</h4>
<p class="text">Optional object filtering based on size and intensity. To specify a lower of upper bound only enter the value <tt>-1</tt>.</p>
<a name="primary_postprocessing_roisize_min">
<h4 class="sub">Min. object size</h4>
<p class="subtext">All objects below that size (pixel) are removed.
<br>(parameter scales square with lens magnification)</p>
</a>
<a name="primary_postprocessing_roisize_max">
<h4 class="sub">Max. object size</h4>
<p class="subtext">All objects above that size (pixel) are removed.
<br>(parameter scales square with lens magnification)</p>
</a>
<a name="primary_postprocessing_intensity_min">
<h4 class="sub">Min. average intensity</h4>
<p class="subtext">All objects below that average intensity (8bit gray values) are removed.</p>
</a>
<a name="primary_postprocessing_intensity_max">
<h4 class="sub">Max. average intensity</h4>
<p class="subtext">All objects above that average intensity (8bit gray values) are removed.</p>
</a>
</a>

<hr></hr>
<h2 class="param">Secondary Channel</h2>

Parameters for an additional (secondary) channel are defined here. Based on object detection results of the primary channel <i>regions</i> can be defined and modified. Every secondary object can have multiple regions from which features are extracted and classification is performed. Each secondary object is associated to exactly one object in the primary channel.

<hr></hr>
<h3 class="param">Parameters</h3>

<a name="secondary_channelid">
<h4>Secondary channel ID</h4>
<p class="text">The secondary channel Id. See <a href="#primary_channelid">primary channel ID</a>.</p>
</a>

<a name="secondary_channel_conversion">
<a name="secondary_normalizeMin">
<a name="secondary_normalizeMax">
<h4>16 to 8 bit conversion</h4>
<p class="text">See <a href="primary_channel_conversion">16 to 8 bit conversion</a> of the primary channel.</p>
</a>
</a>
</a>

<a name="secondary_channel_registration">
<h4>Channel registration</h4>
<p class="text">Registration problems can be corrected by shifting the secondary relative to the primary channel. The image size is reduced to the overlapping region (absolute values of <i>Shift X</i> in width and <i>Shift Y</i> in height). The shift might vary dependent of filters and lenses used at the microscope.</p>
<a name="secondary_channelregistration_x">
<h4 class="sub">Shift X</h4>
<p class="subtext">Pixel shift in x-direction.</p>
</a>
<a name="secondary_channelregistration_y">
<h4 class="sub">Shift Y</h4>
<p class="subtext">Pixel shift in y-direction.</p>
</a>
</a>

<hr></hr>

<h3 class="param">Z-stacks</h3>
<p class="mtext">In case z-stacks have been acquired the 3D data has to be converted to 2D before object detection. Either a z-slice can be selected or a projection is computed on the z-stack.</p>
<a name="secondary_zslice_selection">
<a name="secondary_zslice_selection_slice">
<h4>Z-slice selection</h4>
<p class="text">See <a href="#primary_zslice_selection">z-slice selection</a> of the primary channel.</p>
</a>
</a>
<a name="secondary_zslice_projection">
<a name="secondary_zslice_projection_method">
<a name="secondary_zslice_projection_begin">
<a name="secondary_zslice_projection_end">
<a name="secondary_zslice_projection_step">
<h4>Z-slice projection</h4>
<p class="text">See <a href="#primary_zslice_projection">z-slice projection</a> of the primary channel.</p>
</a>
</a>
</a>
</a>
</a>

<a name="secondary_flat_field_correction">
<h4 class="text">Flat-field correction</h4>
<p class="text">See <a href="#primary_flat_field_correction">Flat-field correction</a> of the primary channel.</p>
</a>

<hr></hr>

<a name="secondary_region_definition">
<h3 class="param">Region definition</h3>
<p class="mtext">Multiple <i>secondary</i> regions per object can be defined from which features are extracted independently. All parameters scale directly with lens magnification.<br/>
<img class="figure" src=":cecog_secondary_regions" align="bottom"/>
</p>

<a name="secondary_regions_expanded">
<h4>Expanded</h4>
<p class="text">Enlarged version of primary regions.

<a name="secondary_regions_expanded_expansionsize">
<h4 class="sub">Expansion size</h4>
<p class="subtext">Pixel increase of the primary region (unchanged for the values &equiv; 0).</p>
</a>
</p>
</a>

<a name="secondary_regions_inside">
<h4>Inside</h4>
<p class="text">Shrinked version of primary regions.

<a name="secondary_regions_inside_shrinkingsize">
<h4 class="sub">Shrinking size</h4>
<p class="subtext">Pixel decrease of the primary region (unchanged for the values &equiv; 0).</p>
</a>
</p>
</a>

<a name="secondary_regions_outside">
<h4>Ouside</h4>
<p class="text">Area outside of the primary regions.

<a name="secondary_regions_outside_expansionsize">
<h4 class="sub">Expansion size</h4>
<p class="subtext">Pixel increase of the primary region (areas are only computed for the values &gt; 0).</p>
</a>
<a name="secondary_regions_outside_separationsize">
<h4 class="sub">Separation size</h4>
<p class="subtext">Pixel distance between primary and outside region (areas are only computed if separation size &lt; expansion size).</p>
</a>
</p>
</a>

<a name="secondary_regions_rim">
<h4>Rim</h4>
<p class="text">Area around the contour of the primary regions.

<a name="secondary_regions_rim_expansionsize">
<h4 class="sub">Expansion size</h4>
<p class="subtext">Pixel increase of the primary region (areas are only computed if expansion or shrinking size &gt; 0).</p>
</a>
<a name="secondary_regions_rim_shrinkingsize">
<h4 class="sub">Shrinking size</h4>
<p class="subtext">Pixel decrease of the primary region (areas are only computed if expansion or shrinking size &gt; 0).</p>
</a>
</p>
</a>
</a>

<hr></hr>
<a name="controlpanel">
<h2>Control Panel</h2>
<p>
Object detection can be tested individually based on the positions and time-points defined in <a href="qrc:/general">General</a>. Only the object detection of the primary channel is executed for <i>Detect primary objects</i>. The primary channel is processed before the secondary channel for <i>Detect secondary objects</i>.
</p>
</a>

<!-- FOOTER -->

</body>
</html>
