<html>
  <head>
    <link rel="stylesheet"  type="text/css" href="help.css">
  </head>
  <body>
    <h1>Classification</h1>
    Detected image objects can be automatically classified based on user training (supervised learning). For that classes need to be defined, samples manually annotated, features extracted, and a classifier trained. All this can be performed in this section.
    <br/>
    Note a classifier can be trained for multiple experiments, but constant imaging and processing conditions are an important requirement to achieve reproducible results.
    <hr/>
    <h4>Workflow for classification in two channels</h4>
    <br/>
    <img class="figure" src=":/cecog_workflow"/>
    <hr></hr>
    <h4>Workflow for classification in the primary and feature readout in the secondary channel</h4>
    <br/>
    <img class="figure" src=":/cecog_workflow_kinetic"/>
    <hr/>

    A folder structure stores all data of a classifier, including sample annotations, picked sample images and corresponding features, and trained models for support vector machines. Classifiers can be trained for primary and secondary channel independently.
    To build a new classifier contour images are annotated by <img class="extlink" src=":extlink"/><a href="http://rsbweb.nih.gov/ij/">ImageJ</a>'s <img class="extlink" src=":extlink"/><a href="http://rsbweb.nih.gov/ij/plugins/cell-counter.html">CellCounter plugin</a> and saved as XML file. CecogAnalyzer requires a definition of classes and can process one or more XML files, for which feature vectors of all annotated objects are picked. A new support vector machine can be trained from that data. <br><br>Using RBF-kernels the two parameters C and gamma are trained by grid search and 5-fold cross-validation. A confusion matrix visualizes the match between human annotation and machine prediction (see screenshots). Contour images color-coded by predicted class labels visualize classification performance on raw image data.
    <br/>
    <br/>
    All data of a classifier is stored in a folder structure. This structure contains:
    <ul>
      <li><tt>class_definition.txt</tt> - A tab-separated text file with 3 columns specifying <b>class label</b>, <b>class name</b>, and <b>class color</b>. Labels can be any unique numbers, names any string which is allowed in folders, and color is a RGB hex-string like #00FF00 (for pure green, e.g. can be copied from the color menu of Adobe Photoshop).
        <br/>This file is needed to pick annotated objects and must be provided by the user.</li>
      <li><tt>annotations/</tt> - Specifies the annotation needed for <i>sample picking</i>. A folder where the <tt>.xml</tt> files from ImageJ's <img class="extlink" src=":extlink"/><a href="http://rsbweb.nih.gov/ij/plugins/cell-counter.html">CellCounter</a> should be stored specifying which objects and their label to train a classifier.
        <br/>Best practice is to run <a href="qrc:/objectdetection">Object Detection</a> first, which generates contour images as output (see <a href="qrc:/output">Output</a>). These files can be loaded as stack (one stack per position and channel) in ImageJ and annotated with CellCounter (make sure that <i>increment=1</i>). It is also possible to load sub-stacks in case of memory problems.
        Position and first time-point of the stack have to be specified in the filename. All files without an underscore at the beginning are considered and all positions and all time-points will be processed for which an annotation exists.
        <br/>Examples:
        <ul>
          <li>Stack of position 13 with frames 1-200. Filename: <tt>P0013__T00001__200.xml</tt> (note that only the P and T are important. P can be a string referring to the data position and T must be the frame number; knowing the number of frames described in the .xml file might be helpful to load data again in CellCounter.)</li>
          <li>Two stacks of position 15:
            <ul>
              <li>with frames 21-200 (180 images). Filename: <tt>P0015__T00021__180.xml</tt></li>
              <li>and with frames 301-500 (200 images). Filename: <tt>P0015__T00301__200.xml</tt></li>
            </ul>
          </li>
        </ul>
        <br/>These files are needed to pick samples for classifier training and must be provided by the user.</li>
      <li><tt>controls/</tt> - A folder with control images generated by sample picking. All annotated objects are visualized by contours in the class color. Can be deleted.</li>
      <li><tt>data/</tt> - All important data files generated by sample picking and classifier training:
        <ul>
          <li><tt>features.arff</tt> - Feature and class information in the <img class="extlink" src=":extlink"/><a href="http://www.cs.waikato.ac.nz/~ml/weka/arff.html">Attribute-Relation File Format (ARFF)</a> extended by additional information as comments. This file is the output of sample picking containing all annotated objects and extracted features.</li>
          <li><tt>features.sparse</tt> - Same as above but as sparse format (does not contain class names, class colors, or feature names). Redundant file which can be deleted.</li>
          <li><tt>features.samples.txt</tt> - Text file indicating class names and filenames of gallery images in the same order as in the files above.</li>
          <li><tt>features.model /.range</tt> - Results of classifier training containing super vector definitions and scale information generated by <img class="extlink" src=":extlink"/><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libSVM</a>.</li>
          <li><tt>features.confusion.txt</tt> - Text file containing the confusion matrix of the trained support vector machine (SVM) obtained by 5-fold cross-validation. The SVM is trained with weights (to compensate unbalanced training data) and probability estimates.</li>
        </ul>
      </li>
      <li><tt>samples/</tt> - Folder structure with class names as sub-folders containing gallery images generated from sample picking. Raw and mask images (suffix <tt>__img</tt> and <tt>__msk</tt>) are written. The prefix corresponds to the filenames in <tt>features.samples.txt</tt>. These files are only for display/visualization purposes.</li>
    </ul>
    <hr></hr>

    <a name="primary_channel">

      <h2 class="param">Primary Channel</h2>
      <h3>Parameters</h3>

      <a name="primary_classification_envpath"></a>
      <h4>Classifier folder</h4>
      <p class="text">The main folder of a classifier. (Do not select the sub-folders e.g. <tt>data</tt>.)
      </p>


      <a name="primary_classification_regionname"></a>
      <h4>Region name</h4>
      <p class="text">For the primary channel is currently only one choice available.</p>
      <hr></hr>

      <a name="secondary_channel"></a>
      <a name="tertiary_channel"></a>
      <h2 class="param">Secondary-/ Tertiary Channel</h2>

      <h3>Parameters</h3>
      <a name="secondary_classification_envpath"></a>
      <a name="tertiary_classification_envpath"></a>
      <h4>Classifier folder</h4>
      <p class="text">See <a href="#primary_classification_envpath">classifier folder</a> of the primary channel.
      </p>

      <a name="tertiary_classification_regionname"></a>
      <a name="secondary_classification_regionname"></a>
      <h4>Region name</h4>
      <p class="text">One of the regions defined in <a href="qrc:/objectdetection#secondary_region_definition">Object Detection</a>, which is used for classification. The region must be enabled. For now, only one region can be used for classification, but features are extracted for all enabled regions in <a href="qrc:/objectdetection#secondary_region_definition">Object Detection</a>.
      </p>

      <a name="merged_channel"></a>
      <h2 class="param">Merged Channel</h2>
      The Merged Channel is a so called <it>virtual</it> or <it>pseudo</it>
      channel. That means there is no segmentation or object detection
      in this channel. The Merged Channel concatenates the features
      extracted from the other channels into a new feature vector of
      higher dimension. Classification takes place in the higher
      dimensional feature space. <br/>

      <h3>Parameters</h3>
      <a name="merged_classification_envpath"></a>
      <h4>Classifier folder</h4>
      <p class="text">See <a href="#primary_classification_envpath">classifier folder</a> of the primary channel.
      </p>

      <a name="merged_primary_region"></a>
      <a name="merged_secondary_region"></a>
      <a name="merged_Tertiary_region"></a>

      <h4>Merged Channels</h4>
      <p class="text">Choose channels whose features shall be
        concatenated to a higher dimensional feature space. Regions
        defined
        in <a href="qrc:/objectdetection#secondary_region_definition">Object
        Detection</a> can be choosen for each channel, but only one
        region for each channel.</p>



      <hr></hr>
      <a name="controlpanel">
        <h3>Control Panel</h3>
        <p>
          These 3 processes are dependent from left to right:
          <ul>
            <li><b>Pick samples</b> - Takes class definition and annotation files, performs object detection and feature
              extraction of the annotated objects, generates .ARFF and .sparse files, and writes gallery images. Necessary step to train a classifier.</li>
            <li><b>Train classifier</b> - A new support vector machine classifier is trained based on the .ARFF file. The classifier is optimized by grid-search (combinations of parameters C and  &gamma; are tested by 5-fold cross-validation) and confusion matrix is updated. Necessary step to automatically predict object labels.</li>
            <li><b>Test classifier</b> - The newly trained classifier can be tested in this step. All detected objects are shown with color-coded contours corresponding to the predicted class label.</li>
          </ul>
        </p>
      </a>
  </body>
</html>
